>[!cite]
>正确的判断来自经验，而经验来自错误的经验。

## 交叉熵损失函数

假设样本标签为$y\in [1,2,\cdots,n]$

交叉熵损失函数最小化等价于最大似然估计最大化。

## Logistic 回归

logistic 回归使用激活函数将线性函数值挤压到$(0,1)$之间。

在这里，我们的激活函数是 logistic 函数，也叫 sigmoid 函数。

损失函数采用交叉熵损失函数。

## Softmax 回归

Softmax 回归也称为多类 Logistic 回归，是它在多分类问题上的推广。

激活函数采用 Softmax 函数，损失函数采用交叉熵损失函数。

另外，Softmax 回归对所有的权重向量减去同一个$\boldsymbol v$，不改变函数值。可以利用这个特性来正则化约束参数来避免数值计算时数据溢出的问题。

## 感知机




