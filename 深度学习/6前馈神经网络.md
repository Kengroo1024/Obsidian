---
ttite: 前馈神经网络
date: 2024-3-24
categories:
  - 深度学习
---

## 激活函数

激活函数的特征

1. 非线性
2. 几乎处处可导
3. 导数要尽量简单
4. 导数不能太大，也不能太小，更不能无界

## S型曲线

### Logistic函数

定义

$$
\mathrm{logistic}(x)=\dfrac{1}{1+\mathrm{e}^{-x}}
$$

### tanh双曲正切函数

定义

$$
\tanh(x)=\dfrac{\mathrm{e}^x-\mathrm{e}^{-x}}{\mathrm{e}^x+\mathrm{e}^{-x}}
$$

有

$$
\tanh(x)=2\ \mathrm{logistic}(x)-1
$$

## 斜坡函数

### Relu函数

$$
\mathrm{Relu}=\max\{0,x\}
$$

### swish函数

$$
\mathrm{swish}(x)=x\cdot \mathrm{logistic}(\beta x)
$$

### Gelu函数

$$
\mathrm{Gelu}(x)=x\cdot \mathrm{erf}(x)
$$

## 前馈神经网络

网络结构为多层感知机，层与层之间为全连接，不同层间不跨越连接。信号单向传播。

第 $l$ 层的输出用 $a^{(l)}$ 表示，维数与该层神经元的个数相同。$a^{(l)}$ 也即是第 $l+1$ 层的输入。

第 $l$ 层每个神经元都有一个权重向量 $w^{(l,k)}$（这是个行向量），还有一个偏置 $b^{(l,k)}$.

这个神经元的净输出就是 $w^{(l,k)}a^{(l-1)}+b^{(l,k)}$. 对整层而言，其净输出综合起来，可以用矩阵表示，就是

$$
W^{(l)}a^{(l-1)}+b^{(l)}
$$

用 $z^{(l)}$ 表示，那么第 $l$ 层实际上的输出就是 $a^{(l)}=f_l(z^{(l)})$。

这样就完成了信号的前向传播，可以看出，其实前馈神经网络的每层就是对前一层的信号进行了一个仿射变换，再进行一个非线性变换。

## 损失函数与参数学习

损失函数一般采用交叉熵损失。

想要优化误差更新参数，就要求误差函数关于各层的权重矩阵和偏置的偏导数，之后求极值或者采用随机梯度下降。但是这个偏导数直接的求法是很困难的，一般而言我们采用反向传播算法求解。

其中的过程不再赘述，有兴趣的读者可以阅读[蒲公英书](https://nndl.github.io/nndl-book.pdf)的第 4.4 节了解详细步骤。

$$
\begin{aligned}
&\dfrac{\partial\mathcal L}{W^{(l)}}=\delta^{(l)}(\alpha^{(l-1)})^T\\
&\dfrac{\partial\mathcal L}{b^{(l)}}=\delta^{(l)}\\
&\delta^{(l)}=\mathrm{diag}(f'_{l}(z^{(l)}))(W^{(l+1)})^T\delta^{(l+1)}\\
&\delta^{(L)}=f'_L(z^{(L)})
\end{aligned}
$$

于是我们可以从最后一层开始逐层求出误差对于各层的权重矩阵和偏置的偏导数。

## 梯度消失和梯度爆炸

从前文的反向传播计算过程可以看出，网络每加深一层，就要乘上一次激活函数的导数，如果激活函数的导数恒大于 1 或恒小于 1，当网络的深度很深时，导数的值就会以指数趋近 0 或无穷，这给我们进行梯度下降算法带来很大的妨碍。
